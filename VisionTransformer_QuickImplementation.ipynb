{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "15a9303d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from timm.models.layers.weight_init import lecun_normal_, trunc_normal_\n",
    "\n",
    "\n",
    "# from timm and modified for Conv1d implementation\n",
    "def init_weights_vit_jax(module: nn.Module, name: str = ''):\n",
    "    \"\"\" ViT weight initialization, matching JAX (Flax) impl \"\"\"\n",
    "    if isinstance(module, nn.Linear) or isinstance(module, nn.Conv1d):\n",
    "        nn.init.xavier_uniform_(module.weight)\n",
    "        if module.bias is not None:\n",
    "            nn.init.normal_(module.bias, std=1e-6) if 'mlp' in name else nn.init.zeros_(module.bias)\n",
    "    elif isinstance(module, nn.Conv2d):\n",
    "        lecun_normal_(module.weight)\n",
    "        if module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "    elif hasattr(module, 'init_weights'):\n",
    "        module.init_weights()\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self,  embed_dim, output_dim, num_heads):\n",
    "        \"\"\"\n",
    "        dim of each head = embed_dim//num_heads (i.e. 64 as default)\n",
    "        \"\"\"\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.dim_each_heads = embed_dim // num_heads\n",
    "\n",
    "        self.to_q = nn.Conv1d(embed_dim, embed_dim, kernel_size=1, bias=True)\n",
    "        self.to_k = nn.Conv1d(embed_dim, embed_dim, kernel_size=1, bias=True)\n",
    "        self.to_v = nn.Conv1d(embed_dim, embed_dim, kernel_size=1, bias=True)\n",
    "\n",
    "        self.dk = float( torch.rsqrt(torch.FloatTensor([self.dim_each_heads])).item())   # dk: dim of q and k\n",
    "\n",
    "        self.out_proj = nn.Conv1d(embed_dim, output_dim, kernel_size=1, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: batch size, embedding dimension, sequence length   (B, N, L)\n",
    "        \"\"\"\n",
    "        assert x.ndim == 3\n",
    "        \n",
    "        B = x.size(0)       # batch size\n",
    "        Lq = x.shape[-1]    # q sequence length\n",
    "        \n",
    "        q = self.to_q(x)\n",
    "        k = self.to_k(x)\n",
    "        v = self.to_v(x)\n",
    "        \n",
    "        q_multihead = q.view(B, self.num_heads, self.dim_each_heads, -1)\n",
    "        k_multihead = k.view(B, self.num_heads, self.dim_each_heads, -1)\n",
    "        v_multihead = v.view(B, self.num_heads, self.dim_each_heads, -1)\n",
    "        \n",
    "        scaled_qk = torch.einsum('bhnl, bhnm->bhlm', q_multihead, k_multihead) * self.dk # (B, Lq, Lk)\n",
    "        attention_weight = torch.softmax(scaled_qk, dim=2)                               # (B, Lq, Lk)\n",
    "        \n",
    "        transformed_cocatenated_heads = torch.einsum('bhlm, bhkm->bhkl', attention_weight, v_multihead).reshape(B, -1, Lq) # concatenation\n",
    "        \n",
    "        return self.out_proj( transformed_cocatenated_heads )       \n",
    "    \n",
    "    \n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self,  embed_dim,  num_heads, ffn_dim):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.mhsa = MultiHeadSelfAttention(embed_dim, embed_dim, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim, elementwise_affine=True, eps=1e-6)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim, elementwise_affine=True, eps=1e-6)\n",
    "        \n",
    "        self.ffn = nn.Sequential( *[nn.Conv1d(embed_dim, ffn_dim, kernel_size=1, bias=True),\n",
    "                                    nn.ReLU(inplace=True),\n",
    "                                    nn.Conv1d(ffn_dim, embed_dim, kernel_size=1, bias=True)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: batch size, embedding dimension, sequence length   (B, N, L)\n",
    "        \"\"\"\n",
    "        assert x.ndim == 3\n",
    "        skip = x\n",
    "        x = self.mhsa(x)\n",
    "        \n",
    "        x = skip + x\n",
    "        x = self.norm1( x.permute(0, 2, 1) ).permute(0, 2, 1)\n",
    "        \n",
    "        skip = x\n",
    "        x = self.ffn( x )\n",
    "        \n",
    "        x = skip + x\n",
    "        x = self.norm2( x.permute(0, 2, 1) ).permute(0, 2, 1)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class VisionTransformerEncoder(nn.Module):\n",
    "    def __init__(self,  embed_dim,  num_heads, mlp_dim):\n",
    "        super(VisionTransformerEncoder, self).__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim, elementwise_affine=True, eps=1e-6)\n",
    "        self.mhsa = MultiHeadSelfAttention(embed_dim, embed_dim, num_heads)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim, elementwise_affine=True, eps=1e-6)\n",
    "        \n",
    "        self.mlp = nn.Sequential(*[nn.Conv1d(embed_dim, mlp_dim, kernel_size=1, bias=True),\n",
    "                                 nn.ReLU(inplace=True),\n",
    "                                 nn.Conv1d(mlp_dim, embed_dim, kernel_size=1, bias=True)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: batch size, embedding dimension, sequence length   (B, N, L)\n",
    "        \"\"\"\n",
    "        assert x.ndim == 3\n",
    "        skip_1 = x\n",
    "        x = self.norm1( x.permute(0, 2, 1) ).permute(0, 2, 1)\n",
    "        x = self.mhsa(x)\n",
    "        x = skip_1 + x\n",
    "\n",
    "        skip_2 = x\n",
    "        x = self.norm2( x.permute(0, 2, 1) ).permute(0, 2, 1)\n",
    "        x = self.mlp(x)\n",
    "        x = skip_2 + x        \n",
    "        \n",
    "        return x\n",
    "    \n",
    "class ImageEmbedding(nn.Module):\n",
    "    def __init__(self,  image_dim=3, embed_dim=512,  patch_size=(16, 16)):\n",
    "        super(ImageEmbedding, self).__init__()\n",
    "        self.proj = nn.Conv2d(image_dim, embed_dim, kernel_size=patch_size, stride=patch_size, padding=0, bias=True)\n",
    "        self.embed_dim= embed_dim       \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: batch size, image_dim, height, width   (B, C, H, W)\n",
    "        \"\"\"\n",
    "        B = x.size(0)\n",
    "        \n",
    "        return self.proj( x ).view(B, self.embed_dim, -1)   \n",
    "    \n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self,  \n",
    "                 image_size=(256, 256), \n",
    "                 patch_size=(16, 16), \n",
    "                 image_dim=3, \n",
    "                 embed_dim=384, \n",
    "                 mlp_dim=1536, \n",
    "                 num_heads=8,\n",
    "                 num_layers=8,\n",
    "                 img_embed=ImageEmbedding,\n",
    "                 global_pool='avg'):\n",
    "        \"\"\"\n",
    "        No dropout implementation for simplicity\n",
    "        \"\"\"\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.image_dim  = image_dim\n",
    "        self.embed_dim  = embed_dim\n",
    "        self.mlp_dim    = mlp_dim\n",
    "        self.num_heads  = num_heads\n",
    "        self.num_layers = num_layers\n",
    "        embed_len = (image_size[0]//patch_size[0]) * (image_size[1]//patch_size[1])\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1,  embed_dim, embed_len+1) * .02)\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, embed_dim, 1))\n",
    "        \n",
    "        trunc_normal_(self.pos_embed, std=.02)\n",
    "        nn.init.normal_(self.cls_token, std=1e-6)\n",
    "        \n",
    "        if img_embed is not None:\n",
    "            self.img_embed = img_embed(image_dim, embed_dim, patch_size)\n",
    "        else:\n",
    "            self.img_embed = nn.Identity()\n",
    "        \n",
    "        self.transformer = nn.ModuleList([VisionTransformerEncoder(embed_dim, num_heads, mlp_dim) for _ in range(num_layers) ])   \n",
    "        self.norm = nn.LayerNorm(embed_dim, elementwise_affine=True, eps=1e-6)\n",
    "        \n",
    "        self.global_pool = global_pool\n",
    "        \n",
    "        # weight init\n",
    "        for name, module in self.named_modules():\n",
    "            init_weights_vit_jax(module, name)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.img_embed(x)\n",
    "        x = torch.cat( [self.cls_token.expand(x.size(0), -1, -1), x], dim=2)  # concatenate cls token\n",
    "        x = x + self.pos_embed                                                # add positional embedding\n",
    "        for layer in self.transformer: \n",
    "            x = layer(x)\n",
    "        x = self.norm( x.permute(0, 2, 1) ).permute(0, 2, 1)\n",
    "        if self.global_pool == 'avg':\n",
    "            cls = x.mean(dim=2)\n",
    "        else:\n",
    "            cls = x[:, :, 0]   # token  in timm\n",
    "        \n",
    "        return  cls\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d6ac64ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 512, 256])\n",
      "torch.Size([4, 512, 256])\n",
      "torch.Size([4, 384])\n"
     ]
    }
   ],
   "source": [
    "ViT_S_config = {'num_layers':12, 'embed_dim': 384, 'mlp_dim': 1536, 'num_heads': 6} \n",
    "\n",
    "tfe = TransformerEncoder(512, 8, 3072)\n",
    "vtfe = VisionTransformerEncoder(512, 8, 3072)\n",
    "ie = ImageEmbedding(image_dim=3, embed_dim=512,  patch_size=(16, 16))\n",
    "vit = VisionTransformer(**ViT_S_config, global_pool='token')\n",
    "x = torch.rand(4, 3, 256, 256)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print( tfe( ie(x) ).size() )\n",
    "    print( vtfe( ie(x) ).size() )\n",
    "    print( vit( x ).size() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "84654585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 21.69M\n",
      "Number of parameters: 85.84M\n",
      "Number of parameters: 14.59M\n",
      "Number of parameters: 1.83M\n"
     ]
    }
   ],
   "source": [
    "# parameter counts\n",
    "ViT_S_config = {'num_layers':12, 'embed_dim': 384, 'mlp_dim': 1536, 'num_heads': 6} \n",
    "ViT_B_config = {'num_layers':12, 'embed_dim': 768, 'mlp_dim': 3072, 'num_heads':12} \n",
    "HIPT256_config = {'num_layers':8, 'embed_dim': 384, 'mlp_dim': 1536,'num_heads':6} \n",
    "HIPT4096_config = {'num_layers':4, 'embed_dim': 192, 'mlp_dim': 768, 'num_heads':6, 'img_embed': None} \n",
    "\n",
    "num_params = 0\n",
    "for params in VisionTransformer(**ViT_S_config, global_pool='token').parameters():\n",
    "    num_params += torch.prod( torch.tensor(params.shape) )\n",
    "print(\"Number of parameters: {:0.2f}M\".format((num_params / 10**6).item()))\n",
    "\n",
    "num_params = 0\n",
    "for params in VisionTransformer(**ViT_B_config, global_pool='token').parameters():\n",
    "    num_params += torch.prod( torch.tensor(params.shape) )\n",
    "print(\"Number of parameters: {:0.2f}M\".format((num_params / 10**6).item()))\n",
    "\n",
    "num_params = 0\n",
    "for params in VisionTransformer(**HIPT256_config, global_pool='token').parameters():\n",
    "    num_params += torch.prod( torch.tensor(params.shape) )\n",
    "print(\"Number of parameters: {:0.2f}M\".format((num_params / 10**6).item()))\n",
    "\n",
    "num_params = 0\n",
    "for params in VisionTransformer(**HIPT4096_config, global_pool='token').parameters():\n",
    "    num_params += torch.prod( torch.tensor(params.shape) )\n",
    "print(\"Number of parameters: {:0.2f}M\".format((num_params / 10**6).item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "122b0df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141.53729248046875\n",
      "153.80113220214844\n",
      "62.70068359375\n",
      "60.295562744140625\n",
      "58.73565673828125\n",
      "71.57267761230469\n",
      "41.55241012573242\n",
      "67.35658264160156\n",
      "53.23797607421875\n",
      "76.87071228027344\n",
      "67.72838592529297\n",
      "24.76553726196289\n",
      "121.81295776367188\n",
      "113.68099975585938\n",
      "101.64875793457031\n",
      "83.54463195800781\n",
      "56.784637451171875\n",
      "50.604331970214844\n",
      "45.24760055541992\n",
      "71.9202880859375\n",
      "0.11235539615154266\n",
      "38.60708999633789\n",
      "15.8598051071167\n",
      "23.179563522338867\n",
      "83.88377380371094\n",
      "31.170677185058594\n",
      "53.966033935546875\n",
      "51.64238357543945\n",
      "38.58306121826172\n",
      "68.44325256347656\n",
      "70.31535339355469\n",
      "63.14631271362305\n",
      "53.127044677734375\n",
      "33.256980895996094\n",
      "66.13126373291016\n",
      "51.48120880126953\n",
      "44.306785583496094\n",
      "59.341514587402344\n",
      "44.36029815673828\n",
      "54.79623794555664\n",
      "57.2401237487793\n",
      "35.839439392089844\n",
      "58.978271484375\n",
      "51.214149475097656\n",
      "28.859167098999023\n",
      "47.431640625\n",
      "47.35696029663086\n",
      "53.12445068359375\n",
      "43.50859069824219\n",
      "30.73584747314453\n",
      "46.30270767211914\n",
      "63.622528076171875\n",
      "48.988704681396484\n",
      "43.19705581665039\n",
      "41.82673263549805\n",
      "35.30830001831055\n",
      "45.23126220703125\n",
      "32.707157135009766\n",
      "25.869678497314453\n",
      "34.3486213684082\n",
      "38.57611083984375\n",
      "40.04004669189453\n",
      "22.422225952148438\n",
      "13.689687728881836\n",
      "40.69752502441406\n",
      "22.002758026123047\n",
      "47.02649688720703\n",
      "29.054115295410156\n",
      "42.38676452636719\n",
      "42.8553466796875\n",
      "42.86021423339844\n",
      "31.747495651245117\n",
      "31.05080223083496\n",
      "33.35314178466797\n",
      "7.998390197753906\n",
      "35.99652862548828\n",
      "34.12363052368164\n",
      "25.50455093383789\n",
      "25.54465675354004\n",
      "34.134613037109375\n",
      "34.350059509277344\n",
      "27.517154693603516\n",
      "24.781187057495117\n",
      "32.26031494140625\n",
      "30.416061401367188\n",
      "34.018951416015625\n",
      "35.553070068359375\n",
      "34.52390670776367\n",
      "31.482275009155273\n",
      "33.25260925292969\n",
      "29.03501319885254\n",
      "34.7173957824707\n",
      "37.6743278503418\n",
      "35.123451232910156\n",
      "34.309322357177734\n",
      "32.78087615966797\n",
      "32.77769470214844\n",
      "35.56034469604492\n",
      "35.57595443725586\n",
      "30.03404998779297\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     57\u001b[0m x \u001b[38;5;241m=\u001b[39m TF\u001b[38;5;241m.\u001b[39mto_pil_image(torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m)\u001b[38;5;241m.\u001b[39mclamp_(\u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m1.0\u001b[39m)) \u001b[38;5;66;03m# start from PIL Image\u001b[39;00m\n\u001b[0;32m     58\u001b[0m x1 \u001b[38;5;241m=\u001b[39m augment(x)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 59\u001b[0m x2 \u001b[38;5;241m=\u001b[39m \u001b[43maugment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     61\u001b[0m s1, s2 \u001b[38;5;241m=\u001b[39m gs(x1), gs(x2)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[1;32mc:\\users\\enigm\\venv\\torch18\\lib\\site-packages\\torchvision\\transforms\\transforms.py:60\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 60\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\users\\enigm\\venv\\torch18\\lib\\site-packages\\torch\\nn\\modules\\module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    888\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 889\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    890\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[0;32m    891\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[0;32m    892\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m    893\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[1;32mc:\\users\\enigm\\venv\\torch18\\lib\\site-packages\\torchvision\\transforms\\transforms.py:1153\u001b[0m, in \u001b[0;36mColorJitter.forward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m   1151\u001b[0m         img \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39madjust_saturation(img, saturation_factor)\n\u001b[0;32m   1152\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m fn_id \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m hue_factor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1153\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madjust_hue\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhue_factor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\users\\enigm\\venv\\torch18\\lib\\site-packages\\torchvision\\transforms\\functional.py:814\u001b[0m, in \u001b[0;36madjust_hue\u001b[1;34m(img, hue_factor)\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[38;5;124;03m\"\"\"Adjust hue of an image.\u001b[39;00m\n\u001b[0;32m    787\u001b[0m \n\u001b[0;32m    788\u001b[0m \u001b[38;5;124;03mThe image hue is adjusted by converting the image to HSV and\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    811\u001b[0m \u001b[38;5;124;03m    PIL Image or Tensor: Hue adjusted image.\u001b[39;00m\n\u001b[0;32m    812\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m--> 814\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_pil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madjust_hue\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhue_factor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    816\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F_t\u001b[38;5;241m.\u001b[39madjust_hue(img, hue_factor)\n",
      "File \u001b[1;32mc:\\users\\enigm\\venv\\torch18\\lib\\site-packages\\torchvision\\transforms\\functional_pil.py:94\u001b[0m, in \u001b[0;36madjust_hue\u001b[1;34m(img, hue_factor)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_mode \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mI\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m'\u001b[39m}:\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n\u001b[1;32m---> 94\u001b[0m h, s, v \u001b[38;5;241m=\u001b[39m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mHSV\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msplit()\n\u001b[0;32m     96\u001b[0m np_h \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(h, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39muint8)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# uint8 addition take cares of rotation across boundaries\u001b[39;00m\n",
      "File \u001b[1;32mc:\\users\\enigm\\venv\\torch18\\lib\\site-packages\\PIL\\Image.py:1046\u001b[0m, in \u001b[0;36mImage.convert\u001b[1;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[0;32m   1043\u001b[0m     dither \u001b[38;5;241m=\u001b[39m FLOYDSTEINBERG\n\u001b[0;32m   1045\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1046\u001b[0m     im \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdither\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1047\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[0;32m   1048\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1049\u001b[0m         \u001b[38;5;66;03m# normalize source image and try again\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Quick DINO implementation\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "from copy import deepcopy\n",
    "import timm\n",
    "\n",
    "HIPT256_config = {'num_layers':8, 'embed_dim': 384, 'mlp_dim': 1536,'num_heads':6} \n",
    "HIPT256_config_timm = {'depth':8, 'embed_dim': 384, 'mlp_ratio': 4.0,'num_heads':6, 'img_size': 256, 'weight_init': 'jax'} \n",
    "\n",
    "m = 0.9    # centering momentum\n",
    "b = 0.996    # ema momentum\n",
    "\n",
    "augment = transforms.Compose([\n",
    "    transforms.ColorJitter(0.2, 0.2, 0.2, 0.01),\n",
    "    transforms.RandomGrayscale(p=0.1),\n",
    "    transforms.RandomSolarize(0.2, p=0.1),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3),\n",
    "])\n",
    "\n",
    "# define device and models\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "gs = VisionTransformer(**HIPT256_config, global_pool='token') # student network\n",
    "#gs = timm.models.vision_transformer.VisionTransformer(**HIPT256_config_timm, global_pool='token')  # comment out if use timm version\n",
    "#gs.head = nn.Identity()                                                                   # comment out if use timm version\n",
    "gt = deepcopy(gs)                                            # teacher network\n",
    "gs.to(device)\n",
    "gt.to(device)\n",
    "\n",
    "for param_s in gs.parameters():\n",
    "    param_s.requires_grad = True\n",
    "\n",
    "for param_t in gt.parameters():\n",
    "    param_t.requires_grad = False\n",
    "    param_t.grad = None\n",
    "\n",
    "# define loss function\n",
    "def H(t, s, C, tps=0.04, tpt=0.04, eps=1e-6, literal=False):\n",
    "    t = t.detach()\n",
    "    t = torch.softmax((C - t)/tpt, dim=1) \n",
    "    if literal:  # unstable and outputs different value than F.log_softmax\n",
    "        s = torch.softmax(s/tps, dim=1) \n",
    "        return - (t*torch.log(s+eps)).sum(dim=1).mean()\n",
    "    else:\n",
    "        return - (t*F.log_softmax((s)/tps, dim=1)).sum(dim=1).mean()\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.AdamW(gs.parameters(), lr=0.0005)\n",
    "\n",
    "# initialize center (C)\n",
    "C = torch.zeros(gs.embed_dim).to(device)\n",
    "\n",
    "# Demo: single loop training\n",
    "while True:\n",
    "    x = TF.to_pil_image(torch.rand(3, 256, 256).clamp_(0.0, 1.0)) # start from PIL Image\n",
    "    x1 = augment(x).unsqueeze(0).to(device)\n",
    "    x2 = augment(x).unsqueeze(0).to(device)\n",
    "\n",
    "    s1, s2 = gs(x1), gs(x2)\n",
    "    with torch.no_grad():\n",
    "        t1, t2 = gt(x1), gt(x2)\n",
    "\n",
    "    loss = H(t1, s2, C)*0.5 + H(t2, s1, C)*0.5\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(gs.parameters(), max_norm=3.0)\n",
    "    optimizer.step() # update gs\n",
    "\n",
    "    # ema: exponential moving average from https://github.com/facebookresearch/moco/blob/main/moco/builder.py\n",
    "    for param_t, param_s in zip(gt.parameters(), gs.parameters()):\n",
    "        param_t.data = param_t.data * b + param_s.data * (1. - b)\n",
    "    # update center\n",
    "    C = m*C + (1-m)*torch.cat([t1, t2], dim=0).mean(dim=0)\n",
    "    \n",
    "    print(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a2045006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 192])\n"
     ]
    }
   ],
   "source": [
    "# end-to-end training,  is it possible ?\n",
    "vit256 = VisionTransformer(**HIPT256_config, global_pool='token')\n",
    "vit4096 = VisionTransformer(**HIPT4096_config, global_pool='token')\n",
    "\n",
    "mlp256_4096 = nn.Sequential(*[ nn.Conv1d(HIPT256_config['embed_dim'], HIPT4096_config['embed_dim'], kernel_size=1, bias=True),\n",
    "                               nn.ReLU(inplace=True),\n",
    "                               nn.Conv1d(HIPT4096_config['embed_dim'], HIPT4096_config['embed_dim'], kernel_size=1, bias=True)])\n",
    "\n",
    "x = torch.rand(1, 3, 4096, 4096)  # 4K image\n",
    "B = x.size(0)\n",
    "\n",
    "tiled_x = x.unfold(2, 256, 256).unfold(3, 256, 256).permute(0, 2, 3, 1, 4, 5).reshape(-1, 3, 256, 256)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print ( vit4096( mlp256_4096( vit256(tiled_x).reshape(B, -1, 256) ) ).size() )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
