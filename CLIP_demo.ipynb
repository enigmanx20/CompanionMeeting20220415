{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "\n",
    "from  torch.utils.data  import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCam200(Dataset):\n",
    "    def __init__(self, dataset_dir, transform=None):\n",
    "        ext = ['.JPG', '.jpg', '.JPEG', '.png']\n",
    "        self.transform = transform\n",
    "        self.root_dir = dataset_dir\n",
    "        self.normals = sorted( os.listdir(os.path.join(self.root_dir, 'normal')) )\n",
    "        self.tumors = sorted( os.listdir(os.path.join(self.root_dir, 'tumor')) )\n",
    "        try: \n",
    "            self.dirs.remove('thumbnail_position_map')\n",
    "        except:\n",
    "            print()\n",
    "        self.data = []\n",
    "        self.label = []\n",
    "        idx = 0\n",
    "        for d in self.normals:\n",
    "            slide_dir = os.path.join(self.root_dir, 'normal', d)\n",
    "            if os.path.isdir(slide_dir):\n",
    "                imgs = sorted( os.listdir(slide_dir) )\n",
    "                for img in imgs:\n",
    "                    if img[img.find('.'): ] in ext:\n",
    "                        self.data.append(os.path.join(slide_dir, img))\n",
    "                        self.label.append(0)\n",
    "                        idx += 1\n",
    "        for d in self.tumors:\n",
    "            slide_dir = os.path.join(self.root_dir, 'tumor', d)\n",
    "            if os.path.isdir(slide_dir):\n",
    "                imgs = sorted( os.listdir(slide_dir) )\n",
    "                for img in imgs:\n",
    "                    if img[img.find('.'): ] in ext:\n",
    "                        self.data.append(os.path.join(slide_dir, img))\n",
    "                        self.label.append(1)\n",
    "                        idx += 1\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        with open(self.data[idx] , 'rb') as f:\n",
    "            img = Image.open(f).convert('RGB')\n",
    "        return self.transform(img), torch.tensor(self.label[idx], dtype=torch.int64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load('ViT-B/32', device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal021_0000026.jpg\n",
      "  Tumor : 0.79\n",
      "  Normal: 0.21\n",
      "normal054_0000014.jpg\n",
      "  Tumor : 0.74\n",
      "  Normal: 0.26\n",
      "tumor007_0000044.jpg\n",
      "  Tumor : 0.69\n",
      "  Normal: 0.31\n",
      "tumor016_0000038.jpg\n",
      "  Tumor : 0.71\n",
      "  Normal: 0.29\n"
     ]
    }
   ],
   "source": [
    "# zero-shot prediction\n",
    "text_inputs = torch.cat([clip.tokenize(f\"a photo of lymph node {c} tissue\") for c in ['tumor', 'normal']]).to(device) \n",
    "for img in os.listdir(\"./images\"):\n",
    "    print(img)\n",
    "    image_input = preprocess(Image.open(os.path.join(\"./images\", img))).unsqueeze(0).to(device) \n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image_input)\n",
    "        text_features = model.encode_text(text_inputs)\n",
    "\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "    print(\"  Tumor : {:.2f}\".format( similarity[0][0].item() ))\n",
    "    print(\"  Normal: {:.2f}\".format( similarity[0][1].item() ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                          | 0/112 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 112/112 [03:00<00:00,  1.61s/it]\n",
      "  0%|                                                                                           | 0/70 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 70/70 [01:51<00:00,  1.59s/it]\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 82.251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    4.3s finished\n"
     ]
    }
   ],
   "source": [
    "# linear probe\n",
    "def get_features(dataset_dir, transform=None):\n",
    "    dataset = PCam200(dataset_dir, transform=transform)\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(DataLoader(dataset, batch_size=256)):\n",
    "            features = model.encode_image(images.to(device))\n",
    "\n",
    "            all_features.append(features)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "    return torch.cat(all_features).cpu().numpy(), torch.cat(all_labels).cpu().numpy()\n",
    "\n",
    "# Calculate the image features\n",
    "train_features, train_labels = get_features(\"./PCam200/train\", preprocess)\n",
    "test_features, test_labels = get_features(\"./PCam200/test\", preprocess)\n",
    "\n",
    "# Perform logistic regression\n",
    "classifier = LogisticRegression(random_state=0, C=0.316, max_iter=1000, verbose=1)\n",
    "classifier.fit(train_features, train_labels)\n",
    "\n",
    "# Evaluate using the logistic regression classifier\n",
    "predictions = classifier.predict(test_features)\n",
    "accuracy = np.mean((test_labels == predictions).astype(np.float)) * 100.\n",
    "print(f\"Accuracy = {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                          | 0/112 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                           | 0/70 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 3  epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 112/112 [03:03<00:00,  1.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 3  epoch\n",
      "2 / 3  epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  1%|█▏                                                                             | 1/70 [09:09<10:32:00, 549.57s/it]\u001b[A\n",
      "  3%|██▎                                                                             | 2/70 [09:11<4:17:36, 227.31s/it]\u001b[A\n",
      "  4%|███▍                                                                            | 3/70 [09:12<2:18:47, 124.29s/it]\u001b[A\n",
      "  6%|████▋                                                                            | 4/70 [09:14<1:23:28, 75.89s/it]\u001b[A\n",
      "  7%|█████▉                                                                             | 5/70 [09:16<53:15, 49.16s/it]\u001b[A\n",
      "  9%|███████                                                                            | 6/70 [09:18<35:13, 33.02s/it]\u001b[A\n",
      " 10%|████████▎                                                                          | 7/70 [09:19<23:55, 22.78s/it]\u001b[A\n",
      " 11%|█████████▍                                                                         | 8/70 [09:21<16:35, 16.06s/it]\u001b[A\n",
      " 13%|██████████▋                                                                        | 9/70 [09:23<11:45, 11.57s/it]\u001b[A\n",
      " 14%|███████████▋                                                                      | 10/70 [09:24<08:30,  8.52s/it]\u001b[A\n",
      " 16%|████████████▉                                                                     | 11/70 [09:26<06:19,  6.43s/it]\u001b[A\n",
      " 17%|██████████████                                                                    | 12/70 [09:28<04:49,  4.99s/it]\u001b[A\n",
      " 19%|███████████████▏                                                                  | 13/70 [09:29<03:47,  3.99s/it]\u001b[A\n",
      " 20%|████████████████▍                                                                 | 14/70 [09:31<03:04,  3.30s/it]\u001b[A\n",
      " 21%|█████████████████▌                                                                | 15/70 [09:33<02:34,  2.81s/it]\u001b[A\n",
      " 23%|██████████████████▋                                                               | 16/70 [09:35<02:14,  2.49s/it]\u001b[A\n",
      " 24%|███████████████████▉                                                              | 17/70 [09:36<01:58,  2.24s/it]\u001b[A\n",
      " 26%|█████████████████████                                                             | 18/70 [09:38<01:48,  2.08s/it]\u001b[A\n",
      " 27%|██████████████████████▎                                                           | 19/70 [09:40<01:40,  1.97s/it]\u001b[A\n",
      " 29%|███████████████████████▍                                                          | 20/70 [09:41<01:35,  1.90s/it]\u001b[A\n",
      " 30%|████████████████████████▌                                                         | 21/70 [09:43<01:31,  1.86s/it]\u001b[A\n",
      " 31%|█████████████████████████▊                                                        | 22/70 [09:45<01:28,  1.83s/it]\u001b[A\n",
      " 33%|██████████████████████████▉                                                       | 23/70 [09:47<01:24,  1.80s/it]\u001b[A\n",
      " 34%|████████████████████████████                                                      | 24/70 [09:48<01:22,  1.79s/it]\u001b[A\n",
      " 36%|█████████████████████████████▎                                                    | 25/70 [09:50<01:20,  1.78s/it]\u001b[A\n",
      " 37%|██████████████████████████████▍                                                   | 26/70 [09:52<01:17,  1.77s/it]\u001b[A\n",
      " 39%|███████████████████████████████▋                                                  | 27/70 [09:54<01:15,  1.75s/it]\u001b[A\n",
      " 40%|████████████████████████████████▊                                                 | 28/70 [09:55<01:12,  1.74s/it]\u001b[A\n",
      " 41%|█████████████████████████████████▉                                                | 29/70 [09:57<01:10,  1.73s/it]\u001b[A\n",
      " 43%|███████████████████████████████████▏                                              | 30/70 [09:59<01:08,  1.72s/it]\u001b[A\n",
      " 44%|████████████████████████████████████▎                                             | 31/70 [10:00<01:06,  1.71s/it]\u001b[A\n",
      " 46%|█████████████████████████████████████▍                                            | 32/70 [10:02<01:05,  1.71s/it]\u001b[A\n",
      " 47%|██████████████████████████████████████▋                                           | 33/70 [10:04<01:03,  1.72s/it]\u001b[A\n",
      " 49%|███████████████████████████████████████▊                                          | 34/70 [10:06<01:01,  1.71s/it]\u001b[A\n",
      " 50%|█████████████████████████████████████████                                         | 35/70 [10:07<01:00,  1.72s/it]\u001b[A\n",
      " 51%|██████████████████████████████████████████▏                                       | 36/70 [10:09<00:58,  1.71s/it]\u001b[A\n",
      " 53%|███████████████████████████████████████████▎                                      | 37/70 [10:11<00:56,  1.70s/it]\u001b[A\n",
      " 54%|████████████████████████████████████████████▌                                     | 38/70 [10:13<00:56,  1.75s/it]\u001b[A\n",
      " 56%|█████████████████████████████████████████████▋                                    | 39/70 [10:14<00:53,  1.73s/it]\u001b[A\n",
      " 57%|██████████████████████████████████████████████▊                                   | 40/70 [10:16<00:51,  1.73s/it]\u001b[A\n",
      " 59%|████████████████████████████████████████████████                                  | 41/70 [10:18<00:49,  1.70s/it]\u001b[A\n",
      " 60%|█████████████████████████████████████████████████▏                                | 42/70 [10:19<00:47,  1.68s/it]\u001b[A\n",
      " 61%|██████████████████████████████████████████████████▎                               | 43/70 [10:21<00:45,  1.70s/it]\u001b[A\n",
      " 63%|███████████████████████████████████████████████████▌                              | 44/70 [10:23<00:44,  1.71s/it]\u001b[A\n",
      " 64%|████████████████████████████████████████████████████▋                             | 45/70 [10:24<00:42,  1.71s/it]\u001b[A\n",
      " 66%|█████████████████████████████████████████████████████▉                            | 46/70 [10:26<00:41,  1.71s/it]\u001b[A\n",
      " 67%|███████████████████████████████████████████████████████                           | 47/70 [10:28<00:39,  1.71s/it]\u001b[A\n",
      " 69%|████████████████████████████████████████████████████████▏                         | 48/70 [10:29<00:37,  1.69s/it]\u001b[A\n",
      " 70%|█████████████████████████████████████████████████████████▍                        | 49/70 [10:31<00:35,  1.69s/it]\u001b[A\n",
      " 71%|██████████████████████████████████████████████████████████▌                       | 50/70 [10:33<00:33,  1.69s/it]\u001b[A\n",
      " 73%|███████████████████████████████████████████████████████████▋                      | 51/70 [10:34<00:31,  1.68s/it]\u001b[A\n",
      " 74%|████████████████████████████████████████████████████████████▉                     | 52/70 [10:36<00:30,  1.68s/it]\u001b[A\n",
      " 76%|██████████████████████████████████████████████████████████████                    | 53/70 [10:38<00:28,  1.68s/it]\u001b[A\n",
      " 77%|███████████████████████████████████████████████████████████████▎                  | 54/70 [10:39<00:26,  1.67s/it]\u001b[A\n",
      " 79%|████████████████████████████████████████████████████████████████▍                 | 55/70 [10:41<00:25,  1.67s/it]\u001b[A\n",
      " 80%|█████████████████████████████████████████████████████████████████▌                | 56/70 [10:43<00:23,  1.68s/it]\u001b[A\n",
      " 81%|██████████████████████████████████████████████████████████████████▊               | 57/70 [10:45<00:21,  1.67s/it]\u001b[A\n",
      " 83%|███████████████████████████████████████████████████████████████████▉              | 58/70 [10:46<00:20,  1.67s/it]\u001b[A\n",
      " 84%|█████████████████████████████████████████████████████████████████████             | 59/70 [10:48<00:18,  1.66s/it]\u001b[A\n",
      " 86%|██████████████████████████████████████████████████████████████████████▎           | 60/70 [10:49<00:16,  1.66s/it]\u001b[A\n",
      " 87%|███████████████████████████████████████████████████████████████████████▍          | 61/70 [10:51<00:14,  1.65s/it]\u001b[A\n",
      " 89%|████████████████████████████████████████████████████████████████████████▋         | 62/70 [10:53<00:13,  1.66s/it]\u001b[A\n",
      " 90%|█████████████████████████████████████████████████████████████████████████▊        | 63/70 [10:54<00:11,  1.67s/it]\u001b[A\n",
      " 91%|██████████████████████████████████████████████████████████████████████████▉       | 64/70 [10:56<00:10,  1.67s/it]\u001b[A\n",
      " 93%|████████████████████████████████████████████████████████████████████████████▏     | 65/70 [10:58<00:08,  1.69s/it]\u001b[A\n",
      " 94%|█████████████████████████████████████████████████████████████████████████████▎    | 66/70 [11:00<00:06,  1.71s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|██████████████████████████████████████████████████████████████████████████████▍   | 67/70 [11:01<00:05,  1.72s/it]\u001b[A\n",
      " 97%|███████████████████████████████████████████████████████████████████████████████▋  | 68/70 [11:03<00:03,  1.72s/it]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 70/70 [11:05<00:00,  9.51s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 epochs of fine-tuning yields,\n",
      "Accuracy = 89.567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# train epochs\n",
    "epochs = 3\n",
    "# batch size, reduce batch_size if out of memory error occurs\n",
    "batch_size = 256\n",
    "\n",
    "# Linear head transfer learning\n",
    "new_model = torch.nn.Sequential( OrderedDict([\n",
    "                                   \n",
    "                                      ('clip',  model.visual),\n",
    "                                      ('head',  torch.nn.Linear(model.visual.proj.size(1), 2, bias=True))\n",
    "                                  ])\n",
    "                               )\n",
    "# initialize bias of added linear layer as zero\n",
    "torch.nn.init.zeros_( new_model.head.bias.data )\n",
    "\n",
    "# cast dtypes weight and bias of added linear layer to model dtype\n",
    "new_model.head.weight.data = new_model[-1].weight.data.to(model.dtype)\n",
    "new_model.head.bias.data   = new_model[-1].bias.data.to(model.dtype)\n",
    "new_model.to(device)\n",
    "    \n",
    "# prepare dataset and dataloader\n",
    "train_set = PCam200(\"./PCam200/train\", transform=preprocess)\n",
    "test_set  = PCam200(\"./PCam200/test\", transform=preprocess)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader  = DataLoader(test_set,  batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# define optimizer and loss function\n",
    "optimizer = torch.optim.SGD([{\"params\": new_model[-1].parameters(), \"lr\": 1e-3},  \n",
    "                             {\"params\": new_model[0].parameters(), \"lr\": 1e-4}],  # smaller learning rate for backbone model\n",
    "                              momentum=0.9)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "train_iterator = tqdm( train_loader )\n",
    "test_iterator  = tqdm( test_loader  )\n",
    "# training\n",
    "for epoch in range(epochs):\n",
    "    print(f\"{epoch} / {epochs}  epoch\")\n",
    "    for batch in train_iterator:\n",
    "        img, label = batch[0].to(model.dtype).to(device), batch[1].to(device)\n",
    "        new_model.train()\n",
    "        \n",
    "        out = new_model(img)\n",
    "        loss = criterion(out, label)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()      # calculate gradients\n",
    "        optimizer.step()     # update parameters\n",
    "        \n",
    "# testing\n",
    "total = len(test_set)\n",
    "correct = 0.0\n",
    "for batch in test_iterator:\n",
    "    img, label = batch[0].to(model.dtype).to(device), batch[1].to(device)\n",
    "    new_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = new_model(img)\n",
    "\n",
    "        _, predicted = torch.max(out, 1)\n",
    "        correct += (predicted == label).sum().item()\n",
    "\n",
    "# Evaluate accuracy on test set\n",
    "accuracy = (correct/total) * 100.\n",
    "print(f\"{epochs} epochs of fine-tuning yields,\")\n",
    "print(f\"Accuracy = {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal021_0000026.jpg\n",
      "  Tumor : 0.36\n",
      "  Normal: 0.64\n",
      "normal054_0000014.jpg\n",
      "  Tumor : 0.23\n",
      "  Normal: 0.77\n",
      "tumor007_0000044.jpg\n",
      "  Tumor : 0.67\n",
      "  Normal: 0.33\n",
      "tumor016_0000038.jpg\n",
      "  Tumor : 0.50\n",
      "  Normal: 0.50\n"
     ]
    }
   ],
   "source": [
    "# zero-shot prediction after fine-tuning\n",
    "text_inputs = torch.cat([clip.tokenize(f\"a photo of lymph node {c} tissue\") for c in ['tumor', 'normal']]).to(device) \n",
    "for img in os.listdir(\"./images\"):\n",
    "    print(img)\n",
    "    image_input = preprocess(Image.open(os.path.join(\"./images\", img))).unsqueeze(0).to(device) \n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image_input)\n",
    "        text_features = model.encode_text(text_inputs)\n",
    "\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "    print(\"  Tumor : {:.2f}\".format( similarity[0][0].item() ))\n",
    "    print(\"  Normal: {:.2f}\".format( similarity[0][1].item() ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 112/112 [03:00<00:00,  1.61s/it]\n",
      "  0%|                                                                                           | 0/70 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 70/70 [01:51<00:00,  1.59s/it]\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 89.521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.4s finished\n"
     ]
    }
   ],
   "source": [
    "# linear probe after fine-tuning\n",
    "\n",
    "# Calculate the image features\n",
    "train_features, train_labels = get_features(\"./PCam200/train\", preprocess)\n",
    "test_features, test_labels = get_features(\"./PCam200/test\", preprocess)\n",
    "\n",
    "# Perform logistic regression\n",
    "classifier = LogisticRegression(random_state=0, C=0.316, max_iter=1000, verbose=1)\n",
    "classifier.fit(train_features, train_labels)\n",
    "\n",
    "# Evaluate using the logistic regression classifier\n",
    "predictions = classifier.predict(test_features)\n",
    "accuracy = np.mean((test_labels == predictions).astype(np.float)) * 100.\n",
    "print(f\"Accuracy = {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
