{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "\n",
    "from  torch.utils.data  import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCam200(Dataset):\n",
    "    def __init__(self, dataset_dir, transform=None):\n",
    "        ext = ['.JPG', '.jpg', '.JPEG', '.png']\n",
    "        self.transform = transform\n",
    "        self.root_dir = dataset_dir\n",
    "        self.normals = sorted( os.listdir(os.path.join(self.root_dir, 'normal')) )\n",
    "        self.tumors = sorted( os.listdir(os.path.join(self.root_dir, 'tumor')) )\n",
    "        try: \n",
    "            self.dirs.remove('thumbnail_position_map')\n",
    "        except:\n",
    "            print()\n",
    "        self.data = []\n",
    "        self.label = []\n",
    "        idx = 0\n",
    "        for d in self.normals:\n",
    "            slide_dir = os.path.join(self.root_dir, 'normal', d)\n",
    "            if os.path.isdir(slide_dir):\n",
    "                imgs = sorted( os.listdir(slide_dir) )\n",
    "                for img in imgs:\n",
    "                    if img[img.find('.'): ] in ext:\n",
    "                        self.data.append(os.path.join(slide_dir, img))\n",
    "                        self.label.append(0)\n",
    "                        idx += 1\n",
    "        for d in self.tumors:\n",
    "            slide_dir = os.path.join(self.root_dir, 'tumor', d)\n",
    "            if os.path.isdir(slide_dir):\n",
    "                imgs = sorted( os.listdir(slide_dir) )\n",
    "                for img in imgs:\n",
    "                    if img[img.find('.'): ] in ext:\n",
    "                        self.data.append(os.path.join(slide_dir, img))\n",
    "                        self.label.append(1)\n",
    "                        idx += 1\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        with open(self.data[idx] , 'rb') as f:\n",
    "            img = Image.open(f).convert('RGB')\n",
    "        return self.transform(img), torch.tensor(self.label[idx], dtype=torch.int64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load('ViT-B/32', device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal021_0000026.jpg\n",
      "  Tumor : 0.79\n",
      "  Normal: 0.21\n",
      "normal054_0000014.jpg\n",
      "  Tumor : 0.74\n",
      "  Normal: 0.26\n",
      "tumor007_0000044.jpg\n",
      "  Tumor : 0.69\n",
      "  Normal: 0.31\n",
      "tumor016_0000038.jpg\n",
      "  Tumor : 0.71\n",
      "  Normal: 0.29\n"
     ]
    }
   ],
   "source": [
    "# zero-shot prediction\n",
    "text_inputs = torch.cat([clip.tokenize(f\"a photo of lymph node {c} tissue\") for c in ['tumor', 'normal']]).to(device) \n",
    "for img in os.listdir(\"./images\"):\n",
    "    print(img)\n",
    "    image_input = preprocess(Image.open(os.path.join(cur_dir, img))).unsqueeze(0).to(device) \n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image_input)\n",
    "        text_features = model.encode_text(text_inputs)\n",
    "\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "    print(\"  Tumor : {:.2f}\".format( similarity[0][0].item() ))\n",
    "    print(\"  Normal: {:.2f}\".format( similarity[0][1].item() ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                          | 0/286 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 286/286 [05:59<00:00,  1.26s/it]\n",
      "  0%|                                                                                          | 0/177 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 177/177 [03:41<00:00,  1.25s/it]\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 82.239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    4.2s finished\n"
     ]
    }
   ],
   "source": [
    "# linear probe\n",
    "def get_features(dataset_dir, transform=None):\n",
    "    dataset = PCam200(dataset_dir, transform=transform)\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(DataLoader(dataset, batch_size=100)):\n",
    "            features = model.encode_image(images.to(device))\n",
    "\n",
    "            all_features.append(features)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "    return torch.cat(all_features).cpu().numpy(), torch.cat(all_labels).cpu().numpy()\n",
    "\n",
    "# Calculate the image features\n",
    "train_features, train_labels = get_features(\"./train\", preprocess)\n",
    "test_features, test_labels = get_features(\"./test\", preprocess)\n",
    "\n",
    "# Perform logistic regression\n",
    "classifier = LogisticRegression(random_state=0, C=0.316, max_iter=1000, verbose=1)\n",
    "classifier.fit(train_features, train_labels)\n",
    "\n",
    "# Evaluate using the logistic regression classifier\n",
    "predictions = classifier.predict(test_features)\n",
    "accuracy = np.mean((test_labels == predictions).astype(np.float)) * 100.\n",
    "print(f\"Accuracy = {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train epochs\n",
    "epochs = 5\n",
    "# batch size, reduce batch_size if out of memory error occurs\n",
    "batch_size = 512\n",
    "\n",
    "# Linear head transfer learning\n",
    "new_model = torch.nn.Sequential( OrderedDict([\n",
    "                                   \n",
    "                                      ('clip',  model.visual),\n",
    "                                      ('head',  torch.nn.Linear(model.visual.proj.size(1), 2, bias=True))\n",
    "                                  ])\n",
    "                               )\n",
    "# initialize bias of added linear layer as zero\n",
    "torch.nn.init.zeros_( new_model.head.bias.data )\n",
    "\n",
    "# cast dtypes weight and bias of added linear layer to model dtype\n",
    "new_model.head.weight.data = new_model[-1].weight.data.to(model.dtype)\n",
    "new_model.head.bias.data   = new_model[-1].bias.data.to(model.dtype)\n",
    "new_model.to(device)\n",
    "    \n",
    "# prepare dataset and dataloader\n",
    "train_set = PCam200(\"K:\\\\Pcam200_new\\\\train\", transform=preprocess)\n",
    "test_set  = PCam200(\"K:\\\\Pcam200_new\\\\test\", transform=preprocess)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader  = DataLoader(test_set,  batch_size=batch_size*2, shuffle=False)\n",
    "\n",
    "# define optimizer and loss function\n",
    "optimizer = torch.optim.SGD([{\"params\": new_model[-1].parameters(), \"lr\": 1e-3},  \n",
    "                             {\"params\": new_model[0].parameters(), \"lr\": 1e-4}],  # smaller learning rate for backbone model\n",
    "                              momentum=0.9)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "train_iterator = tqdm( train_loader )\n",
    "test_iterator  = tqdm( test_loader  )\n",
    "# training\n",
    "for epoch in range(epochs):\n",
    "    for batch in train_iterator:\n",
    "        img, label = batch[0].to(model.dtype).to(device), batch[1].to(device)\n",
    "        new_model.train()\n",
    "        \n",
    "        out = new_model(img)\n",
    "        loss = criterion(out, label)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()      # calculate gradients\n",
    "        optimizer.step()     # update parameters\n",
    "        \n",
    "    # testing\n",
    "    total = len(test_set)\n",
    "    correct = 0.0\n",
    "    for batch in test_iterator:\n",
    "        img, label = batch[0].to(model.dtype).to(device), batch[1].to(device)\n",
    "        new_model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = new_model(img)\n",
    "\n",
    "            _, predicted = torch.max(out, 1)\n",
    "            correct += (predicted == label).sum().item()\n",
    "\n",
    "    # Evaluate accuracy on test set\n",
    "    accuracy = (correct/total) * 100.\n",
    "    print(f\"{epoch} epochs of fine-tuning yields,\")\n",
    "    print(f\"Accuracy = {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
